<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Ruiqi Xian</title>

    <meta name="author" content="Ruiqi Xian">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Ruiqi Xian (先睿奇)
                </p>
                <p>I'm a Fourth-year PhD in Electrical and Computer Engineering at the University of Maryland College Park, where I am involved in advanced research under the guidance of <a href="https://scholar.google.com/citations?user=X08l_4IAAAAJ&hl=en">Dr. Dinesh Manocha</a> at the <a href="https://gamma.umd.edu/">GAMMA Lab</a>. My primary focus lies in the realms of computer vision and robotics, with a specialization in video processing and understanding.
                </p>
                <p>
                Currently, I am working on perception problems from videos captrued by Unmanned Aerial Vehicles(UAVs). Although my research is primarily centered on aerial scene perception, I am also very interested in topics related to Video Foundation Models and Self-Supervised Learning.
                </p>
                <p style="text-align:center">
                  <a href="mailto:rxian@umd.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Ruiqi_Xian_CV_2024_Nov.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?view_op=list_works&hl=en&hl=en&user=YEQBLOsAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/RuiqiXian">Twitter</a> &nbsp/&nbsp
                  <a href="https://github.com/Ricky-Xian">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/ruiqixian.jpg" class="hoverZoomLink">
              </td>
            </tr>
          </tbody></table>


          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>
                    <font color="red">News</font>
                  </heading>
                  <!-- <p align="left">
                    <font color="red"><strong>News:</strong></font>
                  </p> -->
                  <ul>
                    <li><b><span class="tab">(Sep 2024)</span></b> SOAR is on Arxiv! </li> 
                    <li><b><span class="tab">(Aug 2024)</span></b> Finished my internship at NEC Lab! </li> 
                    <li><b><span class="tab">(June 2024)</span></b> PLAR and AGL-Net have been accepted to IROS 2024! </li> 
                    <li><b><span class="tab">(March 2024)</span></b> HallusionBench has been accepted to CVPR2024! </li> 
                    <li><b><span class="tab">(OCT 2023)</span></b> MITFAS and PMI Sampler have been accepted to WACV 2024! </li> 
                    <li><b><span class="tab">(Jan 2023)</span></b> AZTR has been accepted to ICRA 2023! </li> 
                    
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:20px;width:100%">
              <heading>
                <font color="red">Publications</font>
              </heading>
            </td>
          </tr>
        </tbody>
      </table>

        <table
          style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/soar.png" alt="Aerial_Diffusion" style="border-style: none" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2409.18300">
                <papertitle>SOAR: Self-supervision Optimized UAV Action Recognition with Efficient Object-Aware Pretraining</papertitle>
              </a>
              <br>
              <strong>Ruiqi Xian</strong>,
              Xiyang Wu, Tianrui Guan, Xijun Wang, Boqing Gong, Dinesh Manocha
              <br>
              <em>Arxiv</em>
              <br>
          <p>
          A novel Self-supervised pretraining algorithm for aerial footage captured by Unmanned Aerial Vehicles (UAVs) </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/aglnet.png" alt="Aerial_Diffusion" style="border-style: none" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2404.03187">
                <papertitle>AGL-NET: Aerial-Ground Cross-Modal Global Localization with Varying Scales</papertitle>
              </a>
              <br>
              <strong>Ruiqi Xian*</strong>,
              Tianrui Guan*, Xijun Wang, Xiyang Wu, Mohamed Elnoor, Daeun Song, Dinesh Manocha
              <br>
              <em>IROS 2024</em>
              <br>
          <p>
          A novel learning-based method for global localization using LiDAR point clouds and satellite maps  </p>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/plar.png" alt="Aerial_Diffusion" style="border-style: none" width="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2305.12437">
                <papertitle>PLAR: Prompt Learning for Action Recognition</papertitle>
              </a>
              <br>
              <strong>Ruiqi Xian*</strong>,
              Xijun Wang*,
              Tianrui Guan, Dinesh Manocha
              <br>
              <em>IROS 2024</em>
              <br>
          <p>
          A novel action recognition approach that leverages the strengths of prompt learning to optimize the learning process.  </p>
            </td>
          </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/hallusion.png" alt="Aerial_Booth" style="border-style: none" width="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2310.14566">
                  <papertitle>HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination & Visual Illusion in Large Vision-Language Models</papertitle>
                </a>
                <br>Tianrui Guan*, Fuxiao Liu*, Xiyang Wu, <strong>Ruiqi Xian</strong>, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, Tianyi Zhou
                <br>
                <em>CVPR 2024</em>
                <br>
            <p>
                An comprehensive benchmark designed for the evaluation of image-context reasoning, which presents significant challenges to advanced large visual-language models (LVLMs), such as GPT-4V(Vision) and LLaVA-1.5, by emphasizing nuanced understanding and interpretation of visual data.  </p>
              </td>
            </tr>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/pmi.png" alt="DifFAR" style="border-style: none" width="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2304.06866">
                  <papertitle>PMI Sampler: Patch Similarity Guided Frame Selection for Aerial Action Recognition
                  </papertitle>
                </a>
                <br>
                <strong>Ruiqi Xian</strong>,
                Xijun Wang, Divya Kothandaraman, Dinesh Manocha
                <br>
                <em>WACV 2024</em>
                <br>
            <p>
                A frame selection strategy utilizes the motion bias within videos via patch-wise similarity, enabling the selection of motion-salient frames with dynamic background.</p>
              </td>
            </tr>

          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/mitfas.png" alt="SALAD" style="border-style: none" width="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2303.02575">
                  <papertitle>MITFAS: Mutual Information based Temporal Feature Alignment and Sampling for Aerial Video Action Recognition</papertitle>
                </a>
                <br>
                <strong>Ruiqi Xian*</strong>,
                Xijun Wang, Dinesh Manocha
                <br>
                <em>WACV 2024</em>
                <br>
            <p>
            A novel alignment and sampling approach that roots in information theory to handle the viewpoint changes caused by the UAV ego-motions.  </p>
              </td>
            </tr>


          <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/aztr.png" alt="FAR" style="border-style: none" width="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2303.01589">
                  <papertitle>AZTR: Aerial Video Action Recognition with Auto Zoom and Temporal Reasoning
                </papertitle>
                </a>
                <br>
                 <strong>Ruiqi Xian*</strong>, Xijun Wang*, Tianrui Guan, Celso M. de Melo, Stephen M. Nogar, Aniket Bera, Dinesh Manocha
                <br>
                <em>ICRA 2023</em>
                <br>

                <p> A learning-based approach that can be implemented and evaluated both on the desktop with high-end GPUs and on the low power Platforms for robots and drones.
            </p>
            
              </td>
            </tr>

      </td>
    </tr>
  </tbody>
</table>

<table width="80%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr>
    <td>
      <br>
      <p align="center">
        <font size="2">
          Website template borrowed from<a href="https://jonbarron.info/"> Jon
            Barron</a>.</a>
        </font>
      </p>
    </td>
  </tr>
</table>
</body>

</html>